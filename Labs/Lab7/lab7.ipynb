{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fnRQfyWgtnG"
   },
   "source": [
    "# Задание на ЛР\n",
    "Итоговый код для обучения нейросети и оценки ее точности содержится в приложении. Необходимо увеличить количество скрытых слоев до 3-ех, а количество нейронов в этих слоях так, чтобы обеспечить точность работы нейросети не менее 75%. Темы текстов необходимо изменить в соответствии с вариатом:\n",
    "Вариант 5 - rec.autos, rec.sport.hockey, sci.crypt, sci.med, talk.religion.misc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVR4_UaOgtnH"
   },
   "source": [
    "# Выполнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn3He47ogtnJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3y_qSUCngtnO"
   },
   "outputs": [],
   "source": [
    "def get_word_2_index(vocab): \n",
    "    word2index = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word2index[word] = i \n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "dACZQN4MgtnW",
    "outputId": "a16db0cf-8d5e-48b8-faa0-931850918b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total texts in train: 2759\n",
      "total texts in test: 1836\n",
      "text From: ekr@kyle.eitech.com (Eric Rescorla)\n",
      "Subject: Re: After 2000 years, can we say that Christian Morality is\n",
      "Organization: EIT\n",
      "Lines: 29\n",
      "NNTP-Posting-Host: kyle.eitech.com\n",
      "\n",
      "In article <1qjd3o$nlv@horus.ap.mchp.sni.de> frank@D012S658.uucp (Frank O'Dwyer) writes:\n",
      ">In article <sandvik-140493230024@sandvik-kent.apple.com# sandvik@newton.apple.com (Kent Sandvik) writes:\n",
      ">#In article <1qie61$fkt@horus.ap.mchp.sni.de>, frank@D012S658.uucp (Frank\n",
      ">#O'Dwyer) wrote:\n",
      ">#> Objective morality is morality built from objective values.\n",
      ">#\n",
      ">#You now pushed down the defintion of objectivity into realm of\n",
      ">#objective values. So you need to explain that as well, as well\n",
      ">#as the objective sub-parts, the objective atoms, quarks...\n",
      ">Firstly, science has its basis in values, not the other way round.\n",
      "You keep saying that. I do not think it means what you think it\n",
      "means.\n",
      "Perhaps you should explain what you think \"science has it's basis\n",
      "in values\" means. The reason why people DO science is that\n",
      "they value it's results. That does not mean that science has\n",
      "it's basis in values. Any more than DES stops working if I stop\n",
      "valuing my privacy.\n",
      "\n",
      ">So you better explain what objective atoms are, and how we get them\n",
      ">from subjective values, before we go any further.\n",
      "See above.\n",
      "\n",
      "-Ekr\n",
      "\n",
      "\n",
      "-- \n",
      "Eric Rescorla                                     ekr@eitech.com\n",
      "             Would you buy used code from this man?\n",
      "        \n",
      "\n",
      "category: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [\"sci.space\",\"rec.sport.hockey\",\"sci.crypt\", \"sci.med\", \"talk.religion.misc\"] \n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories) \n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print('total texts in train:',len(newsgroups_train.data)) \n",
    "print('total texts in test:',len(newsgroups_test.data)) \n",
    "print('text',newsgroups_train.data[0]) \n",
    "print('category:',newsgroups_train.target[0])\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for text in newsgroups_train.data:\n",
    "  for word in text.split(\" \"):\n",
    "    vocab[word.lower()] += 1\n",
    "for text in newsgroups_test.data:\n",
    "  for word in text.split(\" \"):\n",
    "    vocab[word.lower()] += 1\n",
    "\n",
    "word2index = get_word_2_index(vocab)\n",
    "total_words = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "AIaiM8jOgtnZ",
    "outputId": "7be61265-1ee3-4f90-903a-2e438240bf86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
      "You set: `1.14`. This will be interpreted as: `1.x`.\n",
      "\n",
      "\n",
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.14\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrRlFwO4gtnb"
   },
   "outputs": [],
   "source": [
    "# Параметры обучения\n",
    "learning_rate = 0.01 \n",
    "training_epochs = 15\n",
    "batch_size = 150\n",
    "display_step = 1\n",
    "# Network Parameters\n",
    "n_hidden_1 = 600 # скрытый слой\n",
    "n_hidden_2 = 300 # скрытый слой\n",
    "n_hidden_3 = 150\n",
    "n_input = total_words # количество уникальных слов в наших текстах \n",
    "n_classes = 5 # 5 классов\n",
    "input_tensor = tf.placeholder(tf.float32,[None, n_input],name=\"input\") \n",
    "output_tensor = tf.placeholder(tf.float32,[None, n_classes],name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4I3utU5gtne"
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(input_tensor, weights, biases):\n",
    "    layer_1_multiplication = tf.matmul(input_tensor, weights['h1']) \n",
    "    layer_1_addition = tf.add(layer_1_multiplication, biases['b1']) \n",
    "    layer_1 = tf.nn.relu(layer_1_addition)\n",
    "    \n",
    "    layer_2_multiplication = tf.matmul(layer_1, weights['h2']) \n",
    "    layer_2_addition = tf.add(layer_2_multiplication, biases['b2']) \n",
    "    layer_2 = tf.nn.relu(layer_2_addition)\n",
    "    \n",
    "    layer_3_multiplication = tf.matmul(layer_2, weights['h3']) \n",
    "    layer_3_addition = tf.add(layer_3_multiplication, biases['b3']) \n",
    "    layer_3 = tf.nn.relu(layer_3_addition)\n",
    "    \n",
    "    out_layer_multiplication = tf.matmul(layer_3, weights['out']) \n",
    "    out_layer_addition = out_layer_multiplication + biases['out']\n",
    "    \n",
    "    return out_layer_addition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHlv5UDDgtnf"
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), \n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes])) }\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6d-FexRgtnh"
   },
   "outputs": [],
   "source": [
    "def get_batch(df, i, batch_size):\n",
    "  batches = []\n",
    "  results = []\n",
    "  texts = df.data[i * batch_size:i * batch_size + batch_size] \n",
    "  categories = df.target[i * batch_size:i * batch_size + batch_size]\n",
    "  for text in texts:\n",
    "    layer = np.zeros(total_words, dtype=float) \n",
    "    for word in text.split(\" \"):\n",
    "      layer[word2index[word.lower()]] += 1 \n",
    "    batches.append(layer)\n",
    "  for category in categories:\n",
    "    y = np.zeros((5), dtype=float)\n",
    "    if category == 0: \n",
    "      y[0] = 1.\n",
    "    elif category == 1: \n",
    "      y[1] = 1.\n",
    "    elif category == 2:\n",
    "      y[2] = 1.\n",
    "    elif category == 3:\n",
    "      y[3] = 1.\n",
    "    else:\n",
    "      y[4] = 1.\n",
    "    results.append(y)\n",
    "  return np.array(batches), np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qCsS_VLgtni"
   },
   "outputs": [],
   "source": [
    "prediction = multilayer_perceptron(input_tensor, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "e-KGEh00gtnk",
    "outputId": "39a03778-0705-413d-e144-0d919496ca60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-6a0ae33cb788>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_tensor)) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "3HF7kTTYgtnl",
    "outputId": "3b145069-c602-4070-892e-3732f4f42e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха: 0001 loss= 81499.8946397569670808\n",
      "Обучение завершено!\n",
      "Эпоха: 0002 loss= 13539.8662923177089397\n",
      "Обучение завершено!\n",
      "Эпоха: 0003 loss= 2284.7179705301923605\n",
      "Обучение завершено!\n",
      "Эпоха: 0004 loss= 2288.2652531729804650\n",
      "Обучение завершено!\n",
      "Эпоха: 0005 loss= 1516.4307522508831880\n",
      "Обучение завершено!\n",
      "Эпоха: 0006 loss= 2517.8781060377755239\n",
      "Обучение завершено!\n",
      "Эпоха: 0007 loss= 1392.7277664078605994\n",
      "Обучение завершено!\n",
      "Эпоха: 0008 loss= 1741.0651477177937068\n",
      "Обучение завершено!\n",
      "Эпоха: 0009 loss= 5618.5449407100686585\n",
      "Обучение завершено!\n",
      "Эпоха: 0010 loss= 4785.6064506570492085\n",
      "Обучение завершено!\n",
      "Эпоха: 0011 loss= 1545.4813162485756948\n",
      "Обучение завершено!\n",
      "Эпоха: 0012 loss= 1678.9702631632487737\n",
      "Обучение завершено!\n",
      "Эпоха: 0013 loss= 93.0814882914225308\n",
      "Обучение завершено!\n",
      "Эпоха: 0014 loss= 0.2654427157508003\n",
      "Обучение завершено!\n",
      "Эпоха: 0015 loss= 0.0000000000000000\n",
      "Обучение завершено!\n",
      "Точность: 0.73801744\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "  sess.run(init)\n",
    "  for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(len(newsgroups_train.data)/batch_size)\n",
    "    for i in range(total_batch):\n",
    "      batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
    "      c,_ = sess.run([loss,optimizer], feed_dict={input_tensor: batch_x,output_tensor:batch_y}) # Вычисляем среднее функции потерь\n",
    "      avg_cost += c / total_batch\n",
    "    print(\"Эпоха:\", '%04d' % (epoch+1), \"loss=\", \"{:.16f}\".format(avg_cost))\n",
    "    print(\"Обучение завершено!\")\n",
    "\n",
    "\n",
    "  correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(output_tensor, 1)) # Расчет точности\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "  total_test_data = len(newsgroups_test.target)\n",
    "  batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data)\n",
    "  print(\"Точность:\", accuracy.eval({input_tensor: batch_x_test, output_tensor: batch_y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1xCzPCwhNhr"
   },
   "source": [
    "# Контрольные вопросы\n",
    "1. Какие вы знаете задачи обработки текстов, в чем они заключаются? \n",
    "Классификация (разбиение по темам), кластеризация (семинатический анализ текстов), построение ассоциативных правил, машинный перевод.\n",
    "2. Зачем нужна предобработка текста для машинного обучения?   \n",
    "Машина не умеет работать со словами, поэтому их преобразуют в эмбеддинги. Не всегда важен порядок слов, а когда-то он играет ключевую роль. Поэтому можно по-разному производить предобработку текстов. Данные могут быть зашумлены либо несогласованы.\n",
    "3. Какие виды предобработки текста вы знаете?  \n",
    "Стемминг, Лемматизация, векторизация, дедубликация, \n",
    "4. Что такое стемминг?  \n",
    "Учет словоформ. Отсекание суффиксов и прочих морфем у слова, чтобы не было множества вариаций по сути одного и того же слова.\n",
    "5. Что такое 20 Newsgroups?  \n",
    "Набор текстовых данных, состоящий из примерно 20 тысяч постов по 20 различным темам.\n",
    "6. Чему должно равняться число входных и выходных нейронов в задаче классификации текстов?\n",
    "Выходных должно быть столько, сколько у нас классов.   \n",
    "Входных должно быть столько, сколько уникальых слов содержится в словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBlYt3Wngtnm"
   },
   "source": [
    "# Список литературы\n",
    "[1] Google. Tensorflow. 2018. Feb. url - https://www.tensorflow.org/install/install_windows.    \n",
    "[2] url - https://virtualenv.pypa.io/en/stable/userguide/.    \n",
    "[3] Microsoft. about_Execution_Policies. 2018. url - https://technet.microsoft.com/en- us/library/dd347641.aspx.   \n",
    "[4] Jupyter Project. Installing Jupyter. 2018. url - http://jupyter.org/install."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Зубаиров Валерий Альбертович"
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "lab7.ipynb",
   "provenance": []
  },
  "group": "ИУ5-24М",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "lab_number": 7,
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "title": "Использование нейронных сетей для анализа текста"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
