{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "SAMPLES_PER_EPOCH = 50000\n",
    "EPOCH_NUM = 800\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W) + b)\n",
    "\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "  \n",
    "def get_batch(features,labels,batch_size): \n",
    "    num_images = features.shape[0]\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=batch_size,\n",
    "                           replace=False)\n",
    "    features_batch = features[idx, :, :, :]\n",
    "    labels_batch = labels[idx, :]\n",
    "\n",
    "    return features_batch, labels_batch\n",
    "  \n",
    "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
    "\n",
    "num_train, img_rows, img_cols, img_channels =  train_features.shape\n",
    "num_test =  test_features.shape[0]\n",
    "num_classes = np.unique(train_labels).shape[0]\n",
    "\n",
    "train_features = train_features.astype('float32')/np.max(train_features)\n",
    "test_features = test_features.astype('float32')/np.max(test_features)\n",
    "\n",
    "\n",
    "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "print(train_features.shape, train_labels.shape)\n",
    "\n",
    "x = tf.placeholder(tf.float32 , shape=[None , img_rows,img_cols,img_channels]) \n",
    "y_ = tf.placeholder(tf.float32 , shape=[None, 10]) \n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "test_features, test_labels = get_batch(test_features,test_labels,500)\n",
    "                                       \n",
    "with tf.name_scope('conv_1'): \n",
    "    conv1 = conv_layer(x , shape=[3, 3, img_channels, 64])\n",
    "    conv1_pool = max_pool_2x2( conv1 )\n",
    "\n",
    "with tf.name_scope('conv_2'): \n",
    "    conv2 = conv_layer(conv1_pool , shape=[3, 3, 64, 128])\n",
    "    conv2_pool = max_pool_2x2 ( conv2 )\n",
    "\n",
    "with tf.name_scope('conv_3'): \n",
    "    conv3 = conv_layer(conv2_pool , shape=[3, 3, 128, 256] )\n",
    "    conv3_pool = max_pool_2x2 ( conv3 )\n",
    "    conv3_flat = tf.contrib.layers.flatten(conv3_pool) \n",
    "\n",
    "with tf.name_scope('full_1'): \n",
    "    full_1 = tf.nn.relu(full_layer(conv3_flat , 512))\n",
    "\n",
    "with tf.name_scope('dropout'): \n",
    "    full1_drop = tf.nn.dropout(full_1 , keep_prob=keep_prob) \n",
    "\n",
    "with tf.name_scope('activations'): \n",
    "    y_conv = full_layer(full1_drop , 10) \n",
    "    tf.summary.scalar('cross_entropy_loss',y_conv)\t\n",
    "\n",
    "with tf.name_scope('cross'): \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_conv , labels=y_)) \n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv , 1), tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction , tf.float32))\n",
    "print(test_features.shape, test_labels.shape)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for j in range(EPOCH_NUM):\n",
    "            for i in range(SAMPLES_PER_EPOCH // BATCH_SIZE):\n",
    "                batch_trainf, batch_trainl = get_batch(train_features,train_labels,BATCH_SIZE)\n",
    "                sess.run(train_step , feed_dict={x: batch_trainf, y_: batch_trainl, keep_prob: 0.5}) \n",
    "            batch_trainf, batch_trainl = get_batch(test_features,test_labels,32)\n",
    "            train_accuracy = sess.run(accuracy , feed_dict={x: batch_trainf, y_: batch_trainl, keep_prob: 1.0}) \n",
    "            print(\"time {}, epoch {}, training accuracy {}\".format(time.time() - start_time, j, train_accuracy)) \n",
    "\n",
    "        test_accuracy = np.mean([sess.run(accuracy , feed_dict={x:test_features, y_:test_labels, keep_prob:1.0})])\n",
    "        print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
